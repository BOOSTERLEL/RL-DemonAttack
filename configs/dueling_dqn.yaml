# Dueling DQN Configuration
# Dueling architecture (Wang et al., 2016) with optimized hyperparameters

# Experiment Settings
exp_name: dueling_dqn
total_steps: 2000000
eval_every: 100000
seed: 42
device: cuda

# Environment Settings
terminal_on_life_loss: true
screen_size: 84
frame_stack: 4

# Model Settings
model_type: DuelingDQN  # Using Dueling architecture
reward_mode: clip

# Training Hyperparameters
gamma: 0.99
lr: 0.00025  # Optimized learning rate
batch_size: 64  # Larger batch for stability
replay_size: 200000  # Much larger replay buffer
warmup: 20000
target_update_freq: 2000  # More frequent updates
grad_clip: 10.0

# Exploration Settings
eps_start: 1.0
eps_end: 0.01  # Lower final epsilon
eps_decay_steps: 1000000

# Advanced Features
use_double_dqn: true
use_prioritized_replay: false
per_alpha: 0.6
per_beta_start: 0.4
per_beta_frames: 100000

# Reward Shaping
use_reward_shaping: false
use_life_penalty: false
life_penalty: -1.0
use_streak_bonus: false
streak_window: 12
streak_bonus: 0.1
