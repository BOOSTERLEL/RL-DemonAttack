# Baseline DQN Configuration
# Nature DQN (Mnih et al., 2015) with standard hyperparameters

# Experiment Settings
exp_name: baseline_dqn
total_steps: 500000
eval_every: 50000
seed: 0
device: cuda

# Environment Settings
terminal_on_life_loss: true
screen_size: 84
frame_stack: 4

# Model Settings
model_type: DQN  # Options: DQN, DuelingDQN, NoisyDQN, NoisyDuelingDQN
reward_mode: clip  # Options: clip, scaled, raw

# Training Hyperparameters
gamma: 0.99
lr: 0.0001
batch_size: 32
replay_size: 50000
warmup: 5000
target_update_freq: 10000
grad_clip: 10.0

# Exploration Settings
eps_start: 1.0
eps_end: 0.1
eps_decay_steps: 1000000

# Advanced Features
use_double_dqn: true
use_prioritized_replay: false
per_alpha: 0.6
per_beta_start: 0.4
per_beta_frames: 100000

# Reward Shaping (optional)
use_reward_shaping: false
use_life_penalty: false
life_penalty: -1.0
use_streak_bonus: false
streak_window: 12
streak_bonus: 0.1
